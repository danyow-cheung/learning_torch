> 
# 注意力提示
## 生物学中的注意力提示
### 突出性的非自主性提示
一堆黑白图片里面有彩色图片，视线自觉被吸引到彩色图片

### 依赖于任务的意志提示
一堆黑白图片里面有彩色图片，但是要在黑白图片中寻找信息，视觉会专注于黑白图片

## 查询，键和值
> 首先，考虑一个相对简单的状况，即只使用非自主性提示。要想将选择偏向于感官输入，则可以简单地使用参数化的全连接层，甚至是非参数化的最大汇聚层或平均汇聚层。
>
> 因此，“是否包含自主性提示”将注意力机制与全连接层或汇聚层区别开来。在注意力机制的背景下，<u>自主性 提示被称为查询(query)。</u>给定任何查询，注意力机制通过注意力汇聚(attention pooling)将选择引导至 感官输入(sensory inputs，例如中间特征表示)。在注意力机制中，这些感官输入被称为值(value)。更通 俗的解释，每个值都与一个键(key)配对，这可以想象为感官输入的非自主提示。如 图10.1.3所示，可以通 过设计注意力汇聚的方式，便于给定的查询(自主性提示)与键(非自主性提示)进行匹配，这将引导得出 最匹配的值(感官输入)。





## 注意力的可视化

平均汇聚层可以被视为输入的加权平均层，其中各输入的权重是一样的，实际上，注意力汇聚得到的是加权平均总和值，其中权重是在给定的查询和不同键之间计算得出的。

> attention_viusalize.py

# 注意力汇聚：Nadaraya-Waston核回归 

> attention_nadaraya_watson.ipynb

## 生成数据集

使用非线性函数生成一个人工数据集，其中加入噪声 ę（服从均值为0和标准差为0.5的正态分布）
$$
yi=2sin(xi)+xi^(0.8) + ę
$$


```python
"""生成数据集"""
n_train = 50  #训练样本数
x_train,_ = torch.sort(torch.rand(n_train)*5)#排序后的训练样本

def f(x):
    return 2*torch.sin(x)+x**0.8

y_train = f(x_train)+torch.normal(0.0,0.5,(n_train,))# 训练样本的输出

```

## 平均汇聚

$$
f(x) = 1/n
$$

使用平均值来解决回归问题





## 非参数注意力汇聚

注意力汇聚公式
$$
f(x) = \sum\limits_{i=1}^{n}：a(x,x_{i})y_{i}
$$
`x`是查询 `x_{i},y_{i}`是键值对。

其中`x`是查询，`(xi, yi)`是键值对。<u>注意力汇聚是yi的加权平均</u>。将查询x和键xi之间的 关系建模为 注意力权重(attention weight)α(x, xi)，如 所示，这个权重将被分配给每一个对应值yi。 对于任何查询，模型在所有键值对注意力权重都是一个有效的概率分布:它们是非负的，并且总和为1。



如果一个键xi越是接近给定的查询x，那么分配给这个键对应值yi的注意力权重就会越大，也

就“获得了更多的注意力”。

```python
# X_repeat的形状（n_test,n_train)
# 每一行都包含相同的测试输入（同样的查询）
'''
repeat_interleave重复张量元素
'''
X_repeat = x_test.repeat_interleave(n_train).reshape((-1,n_train))
# x_trian包含键，attention_weights的形状(n_test,n_train)
# 每一行都包含要给定的每个查询的值(y_train)之间分配的注意力权重
attention_weight = nn.functional.softmax(-(X_repeat-x_train)**2/2,dim=1)
# y_hat的每个元素都是值的加权平均值,其中权重是注意力权重
y_hat = torch.matmul(attention_weight,y_train)
plot_kernel_reg(y_hat)
```









## 带参数注意力汇聚

$$
f(x)=\sum\limits_{i=1}^{n}a(x_{i},y_{i})y_{i}
= \sum\limits_{i=1}^{n} \frac{exp(-\frac{1}{2}(x-x_{i})w)^2}{\sum_{j=1}^{n}exp(-\frac{1}{2}(x-x_{i})w)^2)y_{i}}
$$

```python
'''定义模型'''
class NWKernelRegression(nn.Module):
    def __init__(self,**kwargs):
        super().__init__(**kwargs)
        self.w = nn.Parameter(torch.rand(1,),requires_grad=True)
    def forward(self,queries,keys,values):
        # queries和attention_weights的形状为(查询个数，‘键-值’对个数)
        queries = queries.repeat_interleave(keys.shape[1]).reshape((-1,keys.shape[1]))
        self.attention_weights = nn.functional.softmax(
            -((queries-keys)*self.w)**2/2,dim=1
        )
        # value的个数
        return torch.bmm(self.attention_weights.unsqueeze(1),values.unsqueeze(-1)).reshape(-1)
```





# 注意力评分函数 

注意力权重是概率分布，因此加权和其本质上是加 权平均值。



## 掩蔽softmax操作



## 加性注意力



## 缩放点积注意力





# Bahdanau注意力 
# 多头注意力 
# 自注意力和位置编码 
# Transformer
