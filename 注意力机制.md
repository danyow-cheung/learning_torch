> 
# 注意力提示
## 生物学中的注意力提示
### 突出性的非自主性提示
一堆黑白图片里面有彩色图片，视线自觉被吸引到彩色图片

### 依赖于任务的意志提示
一堆黑白图片里面有彩色图片，但是要在黑白图片中寻找信息，视觉会专注于黑白图片

## 查询，键和值
> 首先，考虑一个相对简单的状况，即只使用非自主性提示。要想将选择偏向于感官输入，则可以简单地使用参数化的全连接层，甚至是非参数化的最大汇聚层或平均汇聚层。
>
> 因此，“是否包含自主性提示”将注意力机制与全连接层或汇聚层区别开来。在注意力机制的背景下，<u>自主性 提示被称为查询(query)。</u>给定任何查询，注意力机制通过注意力汇聚(attention pooling)将选择引导至 感官输入(sensory inputs，例如中间特征表示)。在注意力机制中，这些感官输入被称为值(value)。更通 俗的解释，每个值都与一个键(key)配对，这可以想象为感官输入的非自主提示。如 图10.1.3所示，可以通 过设计注意力汇聚的方式，便于给定的查询(自主性提示)与键(非自主性提示)进行匹配，这将引导得出 最匹配的值(感官输入)。





## 注意力的可视化

平均汇聚层可以被视为输入的加权平均层，其中各输入的权重是一样的，实际上，注意力汇聚得到的是加权平均总和值，其中权重是在给定的查询和不同键之间计算得出的。

> attention_viusalize.py

# 注意力汇聚：Nadaraya-Waston核回归 

> attention_nadaraya_watson.ipynb

## 生成数据集

使用非线性函数生成一个人工数据集，其中加入噪声 ę（服从均值为0和标准差为0.5的正态分布）
$$
yi=2sin(xi)+xi^(0.8) + ę
$$


```python
"""生成数据集"""
n_train = 50  #训练样本数
x_train,_ = torch.sort(torch.rand(n_train)*5)#排序后的训练样本

def f(x):
    return 2*torch.sin(x)+x**0.8

y_train = f(x_train)+torch.normal(0.0,0.5,(n_train,))# 训练样本的输出

```

## 平均汇聚

$$
f(x) = 1/n
$$

使用平均值来解决回归问题





## 非参数注意力汇聚

注意力汇聚公式
$$
f(x) = \sum\limits_{i=1}^{n}：a(x,x_{i})y_{i}
$$
`x`是查询 `x_{i},y_{i}`是键值对。

其中`x`是查询，`(xi, yi)`是键值对。<u>注意力汇聚是yi的加权平均</u>。将查询x和键xi之间的 关系建模为 注意力权重(attention weight)α(x, xi)，如 所示，这个权重将被分配给每一个对应值yi。 对于任何查询，模型在所有键值对注意力权重都是一个有效的概率分布:它们是非负的，并且总和为1。



如果一个键xi越是接近给定的查询x，那么分配给这个键对应值yi的注意力权重就会越大，也

就“获得了更多的注意力”。

```python
# X_repeat的形状（n_test,n_train)
# 每一行都包含相同的测试输入（同样的查询）
'''
repeat_interleave重复张量元素
'''
X_repeat = x_test.repeat_interleave(n_train).reshape((-1,n_train))
# x_trian包含键，attention_weights的形状(n_test,n_train)
# 每一行都包含要给定的每个查询的值(y_train)之间分配的注意力权重
attention_weight = nn.functional.softmax(-(X_repeat-x_train)**2/2,dim=1)
# y_hat的每个元素都是值的加权平均值,其中权重是注意力权重
y_hat = torch.matmul(attention_weight,y_train)
plot_kernel_reg(y_hat)
```









## 带参数注意力汇聚

$$
f(x)=\sum\limits_{i=1}^{n}a(x_{i},y_{i})y_{i}
= \sum\limits_{i=1}^{n} \frac{exp(-\frac{1}{2}(x-x_{i})w)^2}{\sum_{j=1}^{n}exp(-\frac{1}{2}(x-x_{i})w)^2)y_{i}}
$$

```python
'''定义模型'''
class NWKernelRegression(nn.Module):
    def __init__(self,**kwargs):
        super().__init__(**kwargs)
        self.w = nn.Parameter(torch.rand(1,),requires_grad=True)
    def forward(self,queries,keys,values):
        # queries和attention_weights的形状为(查询个数，‘键-值’对个数)
        queries = queries.repeat_interleave(keys.shape[1]).reshape((-1,keys.shape[1]))
        self.attention_weights = nn.functional.softmax(
            -((queries-keys)*self.w)**2/2,dim=1
        )
        # value的个数
        return torch.bmm(self.attention_weights.unsqueeze(1),values.unsqueeze(-1)).reshape(-1)
```





# 注意力评分函数 

注意力权重是概率分布，因此加权和其本质上是加 权平均值。



## 掩蔽softmax操作

softmax操作用于输出一个概率分布作为注意力权重。在某些情况下，并非所有的值都应该 被纳入到注意力汇聚中。

> attention_mask_softmax.ipynb

## 加性注意力

当查询和键是不同长度的矢量时，可以使用加性注意力作为评分函数，

> attention_addtivie.py

## 缩放点积注意力

使用点积可以得到计算效率更高的评分函数

> attention_dot.py



# Bahdanau注意力 

在为给定文本序列生成手写的挑战中，Graves设计了一种可微注意力 模型，将文本字符与更⻓的笔迹对⻬，其中对⻬方式仅向一个方向移动。





## 定义注意力解码器

> attention_bahdanau.py



# 多头注意力 

在實踐中，當給定相同的查詢，鍵和值的集合時，我們希望模型可以基於相同的注意力機制學習到不同的行為。



使用獨立學習得到的h組不同的**線性投影**來變換查詢，鍵和值。

這h組變換後的查詢，鍵和值將並行地送到注意力匯聚中，最後將這h個注意力匯聚的輸出拼接在一起，並且可以通過另外一個可以學習的線性投影進行變換，以產生最終的輸出。上述操作被稱為**多頭注意力**



> attention_mutil.py

# 自注意力和位置编码 

## 自注意力

給定一個由詞元組成的輸入序列x1,..xn，該序列的自注意力輸出為一個長度相同的序列y1,..yn



## 比較卷積神經網絡，循環神經網絡和自注意力

比較cnn，rnn和自注意力的架構複雜性，順序操作和最長路徑長度。

順序操作會妨礙並行操作，而任意的序列位置組合之間的路徑越短，則更輕鬆地學習序列中的遠距離依賴關係。



卷积神经网络和自注意力都拥有并行计算的优势，而且自注意力的最大路径⻓度最短。但是因为
其计算复杂度是关于序列⻓度的二次方，所以在很⻓的序列中计算会非常慢。



## 位置編碼

自注意力因為並行計算而放棄來順序操作，為了使用序列的順序信息，通過在輸入表示中添加位置編碼（postional encoding）來注入絕對的或相對的位置信息。位置編碼可以通過學習得到也可以直接固定得到。

> attention_position.py



# Transformer

由編碼器和解碼器組成。

Trans- former的编码器和解码器是基于自注意力的模块叠加而成的，源(输入)序列和目标(输出)序列的嵌入

(embedding)表示将加上位置编码(positional encoding)，再分别输入到编码器和解码器中。

> attention_transformer.py
