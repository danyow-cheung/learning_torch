# 现代循环神经网络
循环神经⽹络在实践中⼀个常⻅问题是数值不稳定性。尽管我们已经应⽤了梯度裁剪等技巧来缓解这
个问题，但是仍需要通过设计更复杂的序列模型来进⼀步处理它。具体来说，我们将引⼊两个⼴泛使⽤的⽹
络，即⻔控循环单元（gated recurrent units，GRU）和 ⻓短期记忆⽹络（long short-term memory，LSTM）。


## 门控循环单元

### 门控隐状态
门控循环单元和普通的循环神经网络之间的关键区别在于：<u>前者支持隐状态的门控，这意味着模型有专门的机制来确定应该何时更新隐状态，以及何时重置隐状态。</u>

**重置门和更新门**
重置门(reset gate)
更新门(update gate)

**候选隐状态**
**隐状态**

门控循环单位具有以下两个显著特征
- 重置门有助于捕获序列中的短期依赖关系
- 更新门有助于捕获序列中的长期依赖关系
  
### 实现代码
> GRU_1.py 

#### 初始化模型参数

从标准差为0.01的高斯分布中提取权重，并将偏置项设为0，超参数num_hiddens定义隐藏单元的数量，实例化与更新门，重置门，候选隐藏状态和输出层相关权重和偏置



## 长短期记忆网络

### 门控记忆元

⻓短期记忆⽹络引⼊了记忆元（memory cell）， 或简称为单元（cell）。有些⽂献认为记忆元是隐状态的⼀种特殊类型，它们与隐状态具有相同的形状，其设 计⽬的是⽤于记录附加的信息。为了控制记忆元，我们需要许多⻔。其中⼀个⻔⽤来从单元中输出条⽬，我 们将其称为输出⻔（output gate）。另外⼀个⻔⽤来决定何时将数据读⼊单元，我们将其称为输⼊⻔（input gate）。我们还需要⼀种机制来重置单元的内容，由遗忘⻔（forget gate）来管理，这种设计的动机与⻔控循 环单元相同，能够通过专⽤机制决定什么时候记忆或忽略隐状态中的输⼊。



#### 输入门，忘记门和输出门

<img src ='img/输入门忘记门输出门.png'>



#### 候选记忆元 

使用`tanh`作为激活函数，函数范围为(-1,1)

#### 记忆元

在GRU中，有一种机制来控制输入和遗忘。类似的，在长短期记忆网络中，也有两个门用于这样的目的。

引入这种设计为缓解梯度消失问题，并更好地捕获序列中的长距离依赖关系。



#### 隐状态



### 实现代码

> GRU_2.py

#### 初始化模型参数

num_hiddens定义隐藏单元的数量，按照标准差0.01的高斯分布初始化权重，并将偏置项设为0



## 深度循环神经网络

GRU和LSTM都是单向隐藏层的循环神经网络。

那么如何在循环神经网络中添加更加多的层，以及在哪里添加额外的非线性？

可以将多层循环神经网络堆叠在一起，通过几个层的组合，产生一个灵活的机制，特别是，数据可能与不同层的堆叠有关。



### 函数依赖关系

<img src='img/深度循环神经网络.png'>

### 简洁实现

> GRU_3.py





## 双向循环神经网络

> 多用于自然语言中的填空题类似

### 隐马尔可夫模型中的动态规划

math goodbye 

### 双向模型 

如果我们希望在循环神经⽹络中拥有⼀种机制，使之能够提供与隐⻢尔可夫模型类似的前瞻能⼒，我们就需 要修改循环神经⽹络的设计。幸运的是<u>，这在概念上很容易，只需要增加⼀个“从最后⼀个词元开始从后向 前运⾏”的循环神经⽹络，⽽不是只有⼀个在前向模式下“从第⼀个词元开始运⾏”的循环神经⽹络。</u>



**双向循环神经网络（bidirectionals RNNs)**添加了反向传递信息的隐藏层。以便更灵活地处理此类信息。







### 双向循环神经网络的错误应用

双向循环神经网络使用了过去和未来的数据，所以我们不能盲目地将一语言模型应用于任何预测任务。

> Bidirectional_1.py



## 机器翻译与数据集

> Translation.py

### 下载和预处理数据集

```python
'''3.19 《机器翻译与数据集》
步骤
1. 下载和预处理数据集
2. 词元化
3. 词表
4. 加载数据集 
5. 训练模型
'''
import os 
import torch 
from d2l import torch as d2l 

# step 1:
d2l.DATA_HUB['fra-eng'] = (d2l.DATA_URL+'fra-eng.zip','94646ad1522d915e7b0f9296181140edcf86a4f5')

def read_data_nmt():
	'''加载英语-法院数据集'''
	data_dir = d2l.download_extract('fra-eng')
	with open(os.path.join(data_dir,'fra.txt'),"r",encoding='utf-8') as f:
		return f.read()

raw_text = read_data_nmt()
print(raw_text[:100])

'''下载数据集后，原始文本数据经过预处理步骤
1. 空格代替不间断空格
2. 使用小写字母代替大写字母
3. 在单词和标点之间添加空格
'''
def preprocess_nmt(text):
	def no_space(char,prev_char):
		return char in set(',.!?') and prev_char != ' '

	# 使用空格代替不间断空格
	# 使用小写字母代替大写字母
	text = text.replace('\u202f',' ').replace('\xa0',' ').lower()

	# 在单词和标点之间添加空格
	out = [" " + char if i>0 and no_space(char,text[i-1]) else char for i,char in enumerate(text)]
	return "".join(out)

text = preprocess_nmt(raw_text)
print(text[:100])
```



!下面的代码放到ubuntu跑多次!

### 词元化

单词级词元化，。下⾯的tokenize_nmt函数对前num_examples个⽂本序列对进⾏词元，其中每个词元要么是 ⼀个词，要么是⼀个标点符号。此



### 词表

### 加载数据集



## 编码器-解码器架构

Encoder-Decoder 

### 编码器（Encoder）

> Encoder.py

### 解码器(Decoder)

> Decoder.py

### 合并编码器和解码器

> Encoder_Decoder.py

## 序列到序列学习

> seq2seq.py



## 束搜索
