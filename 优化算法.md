# 优化算法

## 优化和深度学习

对于深度学习问题，首先定义损失函数，一旦我们有了损失函数，就可以使用优化算法来尝试最小化损失。

**<u>在优化中，损失函数通常被称为优化问题的目标函数。</u>**

### 优化的目标

<u>**优化算法目标是减少训练误差。深度学习的目标是减少泛化误差，**</u>

两者根本是不同的。

> optimize_target.py

引入**概念风险f **和**经验风险g**两个概念。

> 经验风险：训练数据集的平均损失
>
> 风险: 整个数据群的预期损失，

### 深度学习中优化挑战

>  讲模型训练误差

#### 局部最小值

深度学习模型的目标函数通常有许多局部最优解，但优化问题的数值接近局部最优值，随着目标函数解的梯度接近或变为零，通过最终迭代获得数值解可能仅为使目标函数局部最优，而不是全局最优。

<u>只有一定程度的噪声可能会使参数跳出局部最小值。</u>事实上，这是小批量随机梯度下降的有利特性之一。在这种情况下，小批量上梯度的自然变化能够将参数从局部极小值中跳出。



#### 鞍点

鞍点（saddle point）是函数的所有梯度都消失但既不是全局最小值也不是局部最小值的任何位置。



#### 梯度消失





## 凸性

凸性（convexity)在优化算法的设计中起到作用。

> optimize_convexity.py



## 梯度下降

### 随机梯度下降

可以降低每次迭代时的计算代价，在随机梯度下降的每次迭代中，我们对数据样本随机均匀采样一个索引，并计算索引以更新`x`

#### 动态学习率

用与时间相关的学习率η(t)取代η增加了控制优化算法收敛的复杂性，

以下是随着时间推移调整`N`的基本策略

- 分段常数
  $$
  n(t)=n_{i} if t_{i}<t<t_{t+1}
  $$
  

- 指数衰减
  $$
  n(t) = n_{0} * e ^-t
  $$
  

- 多项式衰减
  $$
  n(t) = n_{0}(ßt+1)^-a
  $$
  







## 小批量随机梯度下降



## 动量法



## AdaGrad算法



## RMSprop算法



## Adadelta



## Adam



## 学习率调度器