# 计算机视觉

## 微调

实现步骤

1. 在源数据集(例如ImageNet数据集)上预训练神经网络模型，即源模型。
2. 创建一个新的神经网络模型，即目标模型。这将复制源模型上的所有模型设计及其参数(输出层除外)。 我们假定这些模型参数包含从源数据集中学到的知识，这些知识也将适用于目标数据集。我们还假设 源模型的输出层与源数据集的标签密切相关;因此不在目标模型中使用该层。
3. 向目标模型添加输出层，其输出数是目标数据集中的类别数。然后随机初始化该层的模型参数。
4. 在目标数据集(如椅子数据集)上训练目标模型。输出层将从头开始进行训练，而所有其他层的参数将 根据源模型的参数进行微调。

**当目标数据集比源数据集小得多时，微调有助于提高模型的泛化能力。**

### 热狗识别

> hotdog_recognition.py/hotdog_recognition.ipynb





## 单发多框检测

> ssd.py

### 类别预测层

如果使用全 连接层作为输出，很容易导致模型参数过多。

使用卷积层的通道来输出类别预测的方 法，单发多框检测采用同样的方法来降低模型复杂度。

下面，定义一个类别预测层，通过参数num_anchors和num_classes 分别指定a和q,该图层使用填充为1的3x3卷积层，此卷积层的输入和输出的宽度和高度保持不变。

```python
def cls_predictor(num_inputs,num_anchors,num_classes):
  return nn.Conv2d(num_inputs,num_anchors*(num_classes +1),kernel_size = 3,padding=1)

```



### 边界框预测层

```python
# 和类比预测框类似，唯一不同的是，需要为每个锚框预测4个偏移量，而不是q+1类别
def bbox_predictor(num_inputs,num_anchors):
  return nn.Conv2d(num_inputs,nun_anchors=4,kernel_size=3,padding=1)
```





### 连结多尺度的预测

为同一个小批量构建两个不同比列（Y1和Y2）的特征图，其中Y2的高度和宽度是Y1的一半，

```python
def forward(x,block):
  return block(x)

Y1 =forward(torch.zeros((2,8,20,20)),cls_predictor(8,5,10))

Y2 =forward(torch.zeros((2,16,10,10)),cls_predictor(16,3,10))
print(Y1.shape,Y2.shape)
```





通道维包含中心相同的锚框的预测结果。我们首先将通道维移到最后一维。因为不同尺度下批量大小仍保持不变，我们可以将预测结果转成二维的(批量大小，高×宽×通道数)的格式，以方便之后在维度1上的连结

```python
def flatten_pred(pred):
  return torch.flatten(pred.permute(0,2,3,1),start_dim=1)

def concat_preds(preds):
  return torch.cat([flatten_pred(p) for p in preds],dim=1)


```



### 高和宽减半块

每个高和宽减 半块由两个填充为1的3 × 3的卷积层、以及步幅为2的2 × 2最大汇聚层组成。



对于此高 和宽减半块的输入和输出特征图,所以输出中的每个单元在输入上都有一 个6 × 6的感受野。因此**，高和宽减半块会扩大每个单元在其输出特征图中的感受野。**



### 基本网络块

用于从输入图像中抽取特征，

> 基本网络块用于从输入图像中抽取特征。为了计算简洁，我们构造了一个小的基础网络，该网络串联3个高和
>
> 宽减半块，并逐步将通道数翻倍。给定输入图像的形状为256 × 256，此基本网络块输出的特征图形状为32 × 32 (256/23 = 32)。



### 完整的模型

```python
class TinySSD(nn.Module):
    def __init__(self,num_classes,**kwargs):
        super(TinySSD,self).__init__(**kwargs)
        self.num_classes = num_classes
        idx_to_in_channels = [64,128,128,128,128]
        
        for i in range(5):
            # 即赋值语句
            setattr(self,f'blk_{i}',get_blk(i))
            setattr(self,f'cls_{i}',cls_predictor(idx_to_in_channels[i],num_anchors,num_classes))
            setattr(self,f'bbox_{i}',bbox_predictor(idx_to_in_channels[i],num_anchors))
            # self.blk_i = get_blk(i)

    def forward(self,X):
        anchors,cls_preds,bbox_preds = [None]*5,[None]*5,[None]*5
        for i in range(5):

            X,anchors[i],cls_preds[i],bbox_preds[i] = blk_forward(
                X,getattr(self,f'blk_{i}'),sizes[i],ratios[i],
                getattr(self,f'cls_{i}'),getattr(self,f'bbox_{i}'))

        anchors = torch.cat(anchors,dim=1)

        cls_preds = concat_preds(cls_preds)
        
        print(cls_preds.shape)# add  这一步已经错落

        cls_preds = cls_preds.reshape(cls_preds.shape[0],-1,self.num_classes+1)

        bbox_preds = concat_preds(bbox_preds)
        return anchors,cls_preds,bbox_preds

```





## 区域卷积神经网络(R-CNN)系列

区域卷积神经网络(region-based CNN或regions with CNN features，R-CNN)

> 只提到概念



## 语音分割和数据集

> 包含部分工具代码
>
> segmentation_tool.py





## 转置卷积

如果输入和输出图像的空间维度相同，在以像素级分类的语义分割中将 会很方便。



> transposed_conv.py





## 全卷积网络

全卷积网络将中间层特征图的高和宽变换回输入图像的尺寸:

> fully_conv.py





## 风格迁移

> sty_transform.py

常用的损失函数

1. 内容损失使合成图像与内容图像在内容特征上接近;

2. ⻛格损失使合成图像与⻛格图像在⻛格特征上接近;
3. 全变分损失则有助于减少合成图像中的噪点。







为了抽取图像的内容特征和⻛格特征，我们可以选择VGG网络中某些层的输出。一般来说，<u>越靠近输入层，越 容易抽取图像的细节信息</u>;反之，则越容易抽取图像的全局信息。为了避免合成图像过多保留内容图像的细 节，我们选择VGG较靠近输出的层，即内容层，来输出图像的内容特征。我们还从VGG中选择不同层的输出 来匹配局部和全局的⻛格，这些图层也称为⻛格层。正如 7.2节中所介绍的，VGG网络使用了5个卷积块。实 验中，我们选择第四卷积块的最后一个卷积层作为内容层，选择每个卷积块的第一个卷积层作为⻛格层。这 些层的索引可以通过打印pretrained_net实例获取。







## CIFAR-10 

> CIFAR-10.py



## ImageNet-Dog 

> ImageNet_dog.py

这里微调参数又冻结了参数，到底微调模型要不要冻结模型参数啊😊

<img src =' img/冻结参数.png'>