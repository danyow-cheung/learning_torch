# 自然语言处理

## 预训练

自监督学习(self- supervised learning)已被广泛用于预训练文本表示



许多 较新的预训练模型使相同词元的表示适应于不同的上下文



### 词嵌入

词向量是用于 表示单词意义的向量，并且还可以被认为是单词的特征向量或表示。<u>将单词映射到实向量的技术称为词嵌入。</u>



**<u>独热向量</u>**不是一个好的选择是因为：不能编码词之间的相似性。

针对独热向量的问题。**<u>Word2vec</u>**提出解决方法



Word2Vec工具包含两个模型，即**<u>跳元模型</u>**和**<u>连续词袋</u>**模型。

> which 训练依赖于条件概率，都是自监督模型





### 近似训练

跳元模型的主要思想是使用softmax来计算基于给定的中心词生成上下文的条件概率。

由于softmax操作的性质，上下文词可以是词表中的任意项。但是，求和的梯度很大，为了解决计算的复杂度，介绍两种近似训练的方法，负采样层和分层softmax



#### 负采样





#### 层序softmax



### 用于预训练词嵌入的数据集

> embedding.py





### 预训练word2Vec

> pretrained_word2vec.py



### 全局变量的词嵌入

> GloVe.py

预先计算高频词的词嵌入，提高训练效率



### 子词嵌入



### 词的相似性和类别任务

> word_similar_label.py

### 来自transformers的双向编码

从上下文无关到上下文敏感

> transformers_encode_decode.py



### 用于预训练的BERT的数据集

> pretrained_bert_dataset.py

### 预训练BERT

> pretrained_bert.py





## 应用



## 情感分析

> sentiment_analysis.py
>
> sentiment_analysis2.py



## 自然语言推断

当需要决定一个句子是否可以从另一个句子推断出来，或者需要通过识别语义等价的 句子来消除句子间冗余时，知道如何对一个文本序列进行分类是不够的。相反，我们需要能够对成对的文本 序列进行推断。

> nlp_inference.py



### 微调BERT

> inference_BERT.py	
